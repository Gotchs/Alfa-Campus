{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Библиотеки "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T11:14:09.612517Z",
     "iopub.status.busy": "2022-04-29T11:14:09.611601Z",
     "iopub.status.idle": "2022-04-29T11:14:09.617725Z",
     "shell.execute_reply": "2022-04-29T11:14:09.616729Z",
     "shell.execute_reply.started": "2022-04-29T11:14:09.612464Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras import layers as L\n",
    "from keras.utils import to_categorical\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Получение данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T11:14:09.640173Z",
     "iopub.status.busy": "2022-04-29T11:14:09.639436Z",
     "iopub.status.idle": "2022-04-29T11:14:10.031891Z",
     "shell.execute_reply": "2022-04-29T11:14:10.031043Z",
     "shell.execute_reply.started": "2022-04-29T11:14:09.640141Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('df_train.csv', sep=';')\n",
    "df_test = pd.read_csv('df_test.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T11:14:10.033129Z",
     "iopub.status.busy": "2022-04-29T11:14:10.03292Z",
     "iopub.status.idle": "2022-04-29T11:14:11.483851Z",
     "shell.execute_reply": "2022-04-29T11:14:11.483019Z",
     "shell.execute_reply.started": "2022-04-29T11:14:10.033103Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train['Data'] = df_train.Data.apply(lambda s: list(map(int, s.split(','))))\n",
    "df_train['Target'] = df_train.Target.apply(lambda s: list(map(int, s.split(','))))\n",
    "df_test['Data'] = df_test.Data.apply(lambda s: list(map(int, s.split(','))))\n",
    "\n",
    "# df_test['Target'] = df_test.Data.apply(lambda row: row[-10:])\n",
    "# df_test['Data'] = df_test.Data.apply(lambda row: row[:-10])\n",
    "# all_data = pd.concat([df_train[['Data', 'Target']], df_test[['Data', 'Target']]], axis=0, ignore_index=True)\n",
    "\n",
    "# df_test = pd.read_csv('df_test.csv', sep=';')\n",
    "# df_test['Data'] = df_test.Data.apply(lambda s: list(map(int, s.split(','))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка обучающего датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7033, 500, 186), (7033, 10, 186), (7033, 10, 186))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad = 9999\n",
    "all_codes = list(df_train.Data.explode().unique()) + [pad]\n",
    "vocab = {code: i+1 for i, code in enumerate(all_codes)}\n",
    "decode_vocab = {i: code for code, i in vocab.items()}\n",
    "\n",
    "n_sequences = len(df_train)\n",
    "n_timesteps = 500\n",
    "n_outputs = 10\n",
    "n_features = len(vocab) + 1\n",
    "\n",
    "X1 = np.empty((n_sequences, n_timesteps))\n",
    "X2 = np.empty((n_sequences, n_outputs))\n",
    "y = np.empty((n_sequences, n_outputs))\n",
    "\n",
    "for i in range(n_sequences):\n",
    "    X_seq = df_train.Data[i][-n_timesteps:]\n",
    "    pad_length = n_timesteps - len(X_seq)\n",
    "    X1[i] = [vocab[x] for x in X_seq] + [vocab[pad]] * pad_length\n",
    "    y[i] = [vocab[x] for x in df_train.Target[i]]\n",
    "    X2[i] = [0] + list(y[i][:-1])\n",
    "\n",
    "X1 = to_categorical(X1, num_classes=n_features)\n",
    "X2 = to_categorical(X2, num_classes=n_features)\n",
    "y = to_categorical(y, num_classes=n_features)\n",
    "\n",
    "X1.shape, X2.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Построение и обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "220/220 [==============================] - 574s 3s/step - loss: 2.5236 - accuracy: 0.2949\n",
      "Epoch 2/10\n",
      "220/220 [==============================] - 583s 3s/step - loss: 2.2541 - accuracy: 0.3610\n",
      "Epoch 3/10\n",
      "220/220 [==============================] - 578s 3s/step - loss: 2.1742 - accuracy: 0.3748\n",
      "Epoch 4/10\n",
      "220/220 [==============================] - 573s 3s/step - loss: 2.1312 - accuracy: 0.3774\n",
      "Epoch 5/10\n",
      "220/220 [==============================] - 583s 3s/step - loss: 2.0985 - accuracy: 0.3813\n",
      "Epoch 6/10\n",
      "220/220 [==============================] - 580s 3s/step - loss: 2.0771 - accuracy: 0.3841\n",
      "Epoch 7/10\n",
      "220/220 [==============================] - 579s 3s/step - loss: 2.0582 - accuracy: 0.3868\n",
      "Epoch 8/10\n",
      "220/220 [==============================] - 586s 3s/step - loss: 2.0403 - accuracy: 0.3873\n",
      "Epoch 9/10\n",
      "220/220 [==============================] - 571s 3s/step - loss: 2.0208 - accuracy: 0.3878\n",
      "Epoch 10/10\n",
      "220/220 [==============================] - 564s 3s/step - loss: 2.0071 - accuracy: 0.3892\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1d1112a7a10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define training encoder\n",
    "enc_inputs = L.Input(shape=(None, n_features))\n",
    "encoder_inputs = L.Masking(mask_value=X1[0][-1])(enc_inputs)\n",
    "encoder = L.LSTM(512, return_state=True, dropout=0.3)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# define training decoder\n",
    "dec_inputs = L.Input(shape=(None, n_features))\n",
    "decoder_inputs = L.Masking(mask_value=X1[0][-1])(dec_inputs)\n",
    "decoder_lstm = L.LSTM(512, return_sequences=True, return_state=True, dropout=0.3)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = L.Dense(n_features, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "model = keras.Model([enc_inputs, dec_inputs], decoder_outputs)\n",
    "\n",
    "# define inference encoder\n",
    "encoder_model = keras.Model(enc_inputs, encoder_states)\n",
    "\n",
    "# define inference decoder\n",
    "decoder_state_input_h = L.Input(shape=(512,))\n",
    "decoder_state_input_c = L.Input(shape=(512,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = keras.Model([dec_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "\n",
    "# define model\n",
    "model.compile(optimizer='nadam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train model\n",
    "model.fit([X1, X2], y, epochs=10, batch_size=32, verbose=1, workers=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictioning(seq):\n",
    "    seq = seq[-n_timesteps:]\n",
    "    pad_length = n_timesteps - len(seq)\n",
    "    seq = [vocab[x] for x in seq] + [vocab[pad]] * pad_length\n",
    "    seq = np.array([to_categorical(seq, num_classes=n_features)])\n",
    "    \n",
    "    # encode\n",
    "    state = encoder_model.predict(seq, verbose=0)\n",
    "    \n",
    "    # start of sequence input\n",
    "    target_seq = np.zeros((1, 1, n_features))\n",
    "    \n",
    "    # collect predictions\n",
    "    output = []\n",
    "    \n",
    "    for t in range(n_outputs):\n",
    "        # predict next char\n",
    "        yhat, h, c = decoder_model.predict([target_seq] + state, verbose=0)\n",
    "        # store prediction\n",
    "        output.append(yhat[0,0])\n",
    "        # update state\n",
    "        state = [h, c]\n",
    "        # update target sequence\n",
    "        target_seq = yhat\n",
    "        \n",
    "    return [decode_vocab[np.argmax(x)] for x in output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T11:14:09.622183Z",
     "iopub.status.busy": "2022-04-29T11:14:09.621556Z",
     "iopub.status.idle": "2022-04-29T11:14:09.638102Z",
     "shell.execute_reply": "2022-04-29T11:14:09.637438Z",
     "shell.execute_reply.started": "2022-04-29T11:14:09.622136Z"
    }
   },
   "outputs": [],
   "source": [
    "def apk(actual, predicted, k=10):\n",
    "    if len(predicted) > k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=10):\n",
    "    return np.mean([apk(a, p, k) for a, p in zip(actual, predicted)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11034534473536323\n"
     ]
    }
   ],
   "source": [
    "df_train['Predicted'] = df_train['Data'].apply(predictioning)\n",
    "print(mapk(df_train['Target'], df_train['Predicted']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
